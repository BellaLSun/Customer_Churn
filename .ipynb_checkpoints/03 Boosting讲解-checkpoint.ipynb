{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART: 分类与回归树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类回归树是一棵二叉树，且每个非叶子节点都有两个孩子（孩子结点（child node）：结点的子树的根称为该结点的孩子；叶子结点：也叫终端结点，是度为 0 的结点；），所以对于第一棵子树其叶子节点数比非叶子节点数多1。\n",
    "\n",
    "CART与ID3区别： CART中用于选择变量的不纯性度量是Gini指数； 如果目标变量是标称的，并且是具有两个以上的类别，则CART可能考虑将目标类别合并成两个超类别（双化）； 如果目标变量是连续的，则CART算法找出一组基于树的回归方程来预测目标变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jianshu.com/p/b90a9ce05b28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART算法有两步：\n",
    "\n",
    "#### 决策树生成和剪枝。\n",
    "\n",
    "- 决策树生成：\n",
    "\n",
    "递归地构建二叉决策树的过程，基于训练数据集生成决策树，生成的决策树要尽量大；\n",
    "\n",
    "自上而下从根开始建立节点，在每个节点处要选择一个最好的属性来分裂，使得子节点中的训练集尽量的纯。\n",
    "\n",
    "不同的算法使用不同的指标来定义\"最好\"：\n",
    "\n",
    "- 分类问题，可以选择GINI，双化或有序双化；\n",
    "- 回归问题，可以使用最小二乘偏差（LSD）或最小绝对偏差（LAD）。\n",
    "\n",
    "- 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。\n",
    "\n",
    "这里用代价复杂度剪枝 Cost-Complexity Pruning(CCP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 算法原理和实现\n",
    "ID3算法以信息论为基础，其核心是“信息熵”。ID3算法通过计算每个属性的信息增益，认为信息增益高的是好属性，每次划分选取信息增益最高的属性为划分标准，重复这个过程，直至生成一个能完美分类训练样例的决策树。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='dt.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting 就是通过加入新的弱学习器，来努力纠正前面所有弱学习器的残差(Loss function)，最终这样多个学习器相加在一起用来进行最终预测，准确率就会比单独的一个要高。之所以称为 Gradient，是因为在**添加新模型时使用了梯度下降算法来最小化的损失**。\n",
    "\n",
    "前面已经知道，XGBoost 就是对 gradient boosting decision tree 的实现，但是一般来说，gradient boosting 的实现是比较慢的，因为每次都要先构造出一个树并添加到整个模型序列中。\n",
    "\n",
    "而 XGBoost 的特点就是计算速度快，模型表现好，这两点也正是这个项目的目标。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST原理及缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='xgb.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原理\n",
    "对于上面给出的目标函数，我们可以进一步化简,定义树的复杂度\n",
    "\n",
    "对于f的定义做一下细化，把树拆分成**结构部分q和叶子权重部分w**。下图是一个具体的例子。**结构函数q把输入映射到叶子的索引号上面去**，而w给定了每个索引号对应的**叶子分数**是什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/a819825294/article/details/51206410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种新的定义下，我们可以把目标函数通过泰勒展开，将公式化简为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对$w_j$求导等于0，可以得到:$$w_j = -\\frac{G_j}{H_j+\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终化简得到："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g1,h1...叫做“梯度数据”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分裂节点的两种算法：\n",
    "- 贪心算法\n",
    "- 近似算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贪心算法：\n",
    "缺点：\n",
    "- 在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；\n",
    "- 如果不装进内存，反复地读写训练数据又会消耗非常大的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload-images.jianshu.io/upload_images/536604-bc174faf94fb39eb.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机森林，GBDT，XGBoost的对比\n",
    "**随机森林的集成学习方法是bagging** ，但是和bagging 不同的是bagging只使用bootstrap**有放回的采样样本**，但随机森林即随机采样样本，也随机选择特征，因此**防止过拟合能力更强，降低方差**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有放回抽样\n",
    "如果不放回的话，就很容易造成预测偏差，比如样本不均衡时，如果第一次抽样就已经抽出了少的哪类，剩下的那类几乎都是多的那一类了，在此基础上再进行下几轮的抽样和预测训练的话，就跟少的那类的特征没啥关系了，基于此作出的predict值也肯定不准了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/24851814"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap通过重抽样，可以避免了Cross-Validation造成的样本减少问题，其次，Bootstrap也可以用于创造数据的随机性。比如，我们所熟知的**随机森林算法第一步就是从原始训练数据集中，应用bootstrap方法有放回地随机抽取k个新的自助样本集，并由此构建k棵分类回归树**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap步骤：\n",
    "1. 在原有的样本中通过重抽样抽取一定数量（比如100）的新样本，重抽样（Re-sample）的意思就是有放回的抽取，即一个数据有可以被重复抽取超过一次。\n",
    "\n",
    "2. 基于产生的新样本，计算我们需要估计的统计量。\n",
    "\n",
    "在这例子中，我们需要估计的统计量是$\\alpha$\n",
    "，那么我们就需要基于新样本的计算样本方差、协方差的值作为$\\hat{\\sigma _X^2},\\hat{\\sigma_Y^2}以及\\hat{\\sigma_{XY}}，然后通过上面公式算出一个\\hat{\\alpha}$\n",
    "\n",
    "\n",
    "3. 重复上述步骤n次（一般是n>1000次）。\n",
    "在这个例子中，通过n次（假设n=1000），我们就可以得到1000个$\\alpha_i。也就是\\alpha_1,\\alpha_2,\\cdots,\\alpha_{1000}。$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。\n",
    "\n",
    "- 就是一个在自身样本重采样的方法来估计真实分布的问题\n",
    "\n",
    "- 当我们不知道样本分布的时候，bootstrap方法最有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap(自助法)，Bagging，Boosting(提升)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成学习（ensemble learning）\n",
    "一句话，假设各弱分类器间具有一定差异性（如不同的算法，或相同算法不同参数配置），这会导致生成的分类决策边界不同，也就是说它们在决策时会犯不同的错误。将它们结合后能得到更合理的边界，减少整体错误，实现更好的分类效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先：bagging和boosting都是集成学习（ensemble learning）领域的基本算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Bagging(bootstrap aggregation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging：\n",
    "从训练集进行子抽样组成**每个基模型所需要的子训练集**，对所有基模型预测的结果进行综合,产生最终的预测结果,至于为什么叫bootstrap aggregation，因为它抽取训练样本的时候采用的就是bootstrap的方法！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 从样本集中用Bootstrap采样选出n个训练样本(放回，因为别的分类器抽训练样本的时候也要用)\n",
    "<font color='red'> 但是没有进行对特征的抽样，只有对样本的。</font>\n",
    "2. 在所有属性上，用这n个样本训练分类器（CART or SVM or ...）\n",
    "3. 重复以上两步m次，就可以得到m个分类器（CART or SVM or ...）\n",
    "4. 将数据放在这m个分类器上跑，最后**投票**机制(多数服从少数)看到底分到哪一类(分类问题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">但是bagging和voting又不一样！！（Titanic那个project里有用，要再研究一下）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Bagging代表算法-RF(随机森林)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 用Random(训练样本用Bootstrap方法，选择分离叶子节点用随机选择特征的方式构造一棵决策树(CART)\n",
    "2. 用1的方法构造很多决策树,每棵决策树都最大可能地进行生长而不进行剪枝，许多决策树构成一片森林，**决策树之间没有联系**\n",
    "3. 测试数据进入每一棵决策树，每棵树做出自己的判断，然后进行**投票**选出最终所属类别(默认每棵树权重一致)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### RF优点：\n",
    "1. 不容易出现过拟合，因为选择训练样本的时候就不是全部样本。\n",
    "2. 可以既可以处理属性为离散值的量，比如ID3算法来构造树，也可以处理属性为连续值的量，比如C4.5算法来构造树。\n",
    "3. 对于高维数据集的处理能力令人兴奋，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法。此外，该模型能够输出变量的重要性程度（每个叶子的w），这是一个非常便利的功能。\n",
    "4. 分类不平衡的情况时，随机森林能够提供平衡数据集误差的有效方法，因为是多个decision boundary的组合，会将少量的样本少的类别也照顾的较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF缺点：\n",
    "随机森林在解决回归问题时并没有像它在分类中表现的那么好，这是因为**它并不能给出一个连续型的输出**。当进行回归时，随机森林不能够作出超越训练集数据范围的预测，这可能导致在对某些还有特定噪声的数据进行建模时出现过度拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "核心：Boosting是一种**框架算法**，用来**提高弱分类器准确度**的方法，这种方法通过构造一个预测函数序列，然后以一定的方式将他们组合成为一个准确度较高的预测函数，还有就是，Boosting算法更加关注错分的样本，这点和Active Learning的寻找最有价值的训练样本有点遥相呼应的感觉。              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting算法代表--Adaboost(Adaptive Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 一种迭代算法，针对**同一个训练集**训练**不同的分类器(弱分类器)**\n",
    "2. 然后进行分类，对于分类正确的样本权值低，**分类错误的样本权值高（通常是边界附近的样本）**，最后的分类器是很多弱分类器的线性叠加（**加权组合**，<font color=\"red\">根据谁作为权重？下边就讲了。</font>），分类器相当简单。实际上就是一个简单的弱分类算法提升(boost)的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wizardforcel.gitbooks.io/dm-algo-top10/content/adaboost.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ada.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**更新权重的方法：**\n",
    "\n",
    "该方法只与上一轮样本权重和弱分类器的权重 alpha 值有关。具体计算公式为weighti = weight[i]*exp(-alpha*flag)，可以看出若样本分类正确，则flag为1，exp(-alpha*flag)越小，则新权重越小；若样本分类错误，则flag为-1，exp(-alpha*flag)越大，则新权重会越大。这会使得上一轮分类错误地样本的权重变大，使其在新一轮训练中得到重视。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calcNewWeight(alpha,ygx, weight, gx, y):\n",
    "    newWeight = []\n",
    "    sumWeight = 0\n",
    "    for i in range(len(weight)):\n",
    "        flag = 1\n",
    "        if i  < gx and y[i] != ygx: flag = -1\n",
    "        if i > gx and y[i] != -ygx: flag = -1\n",
    "        # 这种方法是双向更新weight，分对的变小，分错的变大。和上边那种分对了不变的算法不一样。\n",
    "        weighti = weight[i]*exp(-alpha*flag)\n",
    "        newWeight.append(weighti)\n",
    "        sumWeight += weighti\n",
    "    newWeight = newWeight / sumWeight\n",
    " \n",
    "    return newWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ada1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 迭代终止条件\n",
    "不断重复1,2,3步骤，直到达到终止条件为止。也不可能让所有点都被分对，y=h，所以终止条件是强分类器的错误率低于最低错误率阈值或达到最大迭代次数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost 不止适用于分类模型，也可以用来训练回归模型。这需要将弱分类器替换成回归模型，并改动损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost优点\n",
    "1. 可以使用各种方法构造子分类器，Adaboost算法提供的是框架\n",
    "2. 简单，不用做特征筛选\n",
    "3. 相比较于RF，更不用担心过拟合问题（Bella thinks：因为是多个算法组合，所以相对于一个模型，多次取样的RF来说，更不会出现过拟合问题。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost缺点\n",
    "1. 从wiki上介绍的来看，adaboost对于噪音数据和异常数据是十分敏感的。Boosting方法本身**对噪声点异常点很敏感**，因此在每次迭代时候会给噪声点较大的权重，这不是我们系统所期望的。<br>但是对比样本极不均衡的点，或者是更加在意某一类点，觉得将少数点全部分对要纳入一些异常点更重要的case，Adaboost就很合适，比如对customer churn，多纳入一些本来不会churn但是却被预测为churn的datapoint并不很在意。\n",
    "2. 运行速度慢，凡是涉及迭代的基本上都无法采用并行计算，Adaboost是一种\"串行\"算法.\n",
    "3. 所以GBDT(Gradient Boosting Decision Tree)也非常慢。\n",
    "4. AdaBoost 算法只直接支持二分类，遇到多分类的情况，需要借助 one-versus-rest 的思想来训练多分类模型。关于 one-verus-rest 的细节可以参考本系列第一篇文章 SVM。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion：\n",
    "1. Bagging： 树\"并行\"生成 ,如RF;Boosting：树\"串行\"生成,如Adaboost，第一个模型在train dataset上预测完数据之后，针对分错的样本提升weight，再在下一个模型上着重分类。所以是串行结构。\n",
    "\n",
    "2. boosting中的基模型为弱模型，而RF中的基树是强模型(大多数情况)\n",
    "\n",
    "3. boosting重采样的不是样本，而是样本的分布，每次迭代之后，样本的分布会发生变化，也就是被分错的样本会更多的出现在下一次训练集中\n",
    "\n",
    "4. AdaBoost 算法只直接支持二分类，遇到多分类的情况，需要借助 one-versus-rest 的思想来训练多分类模型。关于 one-verus-rest 的细节可以参考本系列第一篇文章 SVM。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种分类方法：Adaboost基于树模型，弱分类器每次都是树模型，每次训练的时候样本都是一样的，只是特征选取上不一样，因为每次迭代完数据点的权重会更新，所以每次训练时选取切分的特征也不一样：还附了代码。\n",
    "https://www.ibm.com/developerworks/cn/analytics/library/machine-learning-hands-on6-adaboost/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面试整理：RF、GBDT、XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提到随机森林，就不得不提Bagging，Bagging可以简单的理解为：放回抽样，**多数表决（分类）或简单平均（回归）**,同时Bagging的基学习器之间属于**并列生成，并行**，不存在强依赖关系。 \n",
    "\n",
    "Random Forest（随机森林）是Bagging的扩展变体，它在以决策树 为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中**引入了随机特征选择**，因此可以概括RF包括四个部分：\n",
    "\n",
    "1. 随机选择样本（放回抽样）；\n",
    "2. 随机选择特征；\n",
    "3. 构建决策树；\n",
    "4. 随机森林投票（平均）。 \n",
    "　\n",
    " 随机选择样本和Bagging相同，随机选择特征是指在树的构建中，会从样本集的特征集合中随机选择部分特征，然后再从这个子集中选择最优的属 性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。 \n",
    "\n",
    "在构建决策树的时候，RF的每棵决策树都**最大可能的进行生长而不进行剪枝,因为只是部分样本，不不太可能过拟合**；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计**，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”。 \n",
    "\n",
    "RF和Bagging对比：RF的起始性能较差，特别当只有一个基学习器时，随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为**在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优缺点\n",
    "随机森林的优点较多，简单总结：\n",
    "1、在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）；\n",
    "\n",
    "2、能够处理很高维的数据，并且不用特征选择，而且在训练完后，给出特征的重要性；\n",
    "\n",
    "3、容易做成并行化方法。 \n",
    "\n",
    "RF的缺点：在噪声较大的分类或者回归问题上回过拟合。**虽然在每棵决策树上不用剪枝，不担心过拟合，但是因为也正因为每棵决策树都不剪枝，合起来就会出现过拟合的结果**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT\n",
    "提GBDT之前，谈一下Boosting，Boosting是一种与Bagging很类似的技术。不论是Boosting还是Bagging，所使用的**多个分类器类型都是一致的**，比如bagging的代表RF中的多个分类器就都是决策树，boosting也是，里边可以套任意的基分类器。\n",
    "\n",
    "但是Boosting当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。 \n",
    "\n",
    "由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，**Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原理\n",
    "　　GBDT与传统的Boosting区别较大，它的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以**在残差减小的梯度方向上建立模型,所以说，在GradientBoost中，每个新的模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting（比如Adaboost）中关注正确错误的样本加权有着很大的区别。 \n",
    "  \n",
    "　　在GradientBoosting算法中，关键就是**利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树，回归哦！**。\n",
    "  \n",
    "　　GBDT的会**累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树）。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优缺点\n",
    "GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显，\n",
    "  1. 它能灵活的处理各种类型的数据；\n",
    "  2. 在相对较少的调参时间下，预测的准确度较高。 \n",
    "\n",
    "当然由于它是Boosting，因此基学习器之前存在串行关系，难以并行训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "## 原理\n",
    "　　XGBoost的性能在GBDT上又有一步提升，而其性能也能通过各种比赛管窥一二。坊间对XGBoost最大的认知在于其能够**自动地运用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高**。<br> \n",
    "　　由于GBDT在合理的参数设置下，往往要生成一定数量的树才能达到令人满意的准确率，在数据集较复杂时，模型可能需要几千次迭代运算。但是XGBoost利用并行的CPU更好的解决了这个问题。 <br>\n",
    "　　其实XGBoost和GBDT的差别也较大，这一点也同样体现在其性能表现上，详见XGBoost与GBDT的区别。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT和XGBoost区别\n",
    "1. 传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑回归（分类）或者线性回归（回归）；\n",
    "- **传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数**；\n",
    "- XGBoost在**代价函数中加入了正则项，用于控制模型的复杂度**。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，**防止过拟合**，这也是XGBoost优于传统GBDT的一个特性；\n",
    "- shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；\n",
    "- 列抽样。XGBoost借鉴了随机森林的做法，支持**列抽样**（抽取特征。行抽样是抽取样本），不仅防止过 拟合，还能减少计算；\n",
    "- 对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动 学习出它的分裂方向；**就不用手工填上特征值了，因为手工也是用RF，boost等这类以随机抽样为基础的算法进行填充。**\n",
    "- XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。**XGBoost的并行是在特征粒度上的**。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么**各个特征的增益计算就可以开多线程进行**。\n",
    "- 可并行的**近似直方图算法**。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，**贪心算法**效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。xgboost的目标函数如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个公式形式上**跟ID3算法、CART算法是一致的**，都是用分裂后的某种值减去分裂前的某种值，从而得到增益。\n",
    "\n",
    "为了限制树的生长，我们可以加入阈值，当增益大于阈值时才让节点分裂，**上式中的gamma即阈值，它是正则项里叶子节点数T的系数，所以xgboost在优化目标函数(求信息增益Gain的时候)的同时相当于做了预剪枝**。\n",
    "\n",
    "另外，上式中还有一个系数lambda，是正则项里leaf score的L2模平方的系数，对**leaf score做了平滑，也起到了防止过拟合的作用，这个是传统GBDT里不具备的特性**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost优于GBDT的方面：\n",
    "- 显式地将树模型的复杂度作为正则项加在优化目标\n",
    "- 公式推导里用到了二阶导数信息，而普通的GBDT只用到一阶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost注意事项：\n",
    "- 多类别分类时，类别需要从0开始编码\n",
    "- Watchlist不会影响模型训练。\n",
    "- XGBoost的特征重要性是如何得到的？某个特征的重要性（feature score），等于它被选中为树节点分裂特征的次数的和，比如特征A在第一次迭代中（即第一棵树）被选中了1次去分裂树节点，在第二次迭代被选中2次…..那么最终特征A的feature score就是 1+2+…."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
