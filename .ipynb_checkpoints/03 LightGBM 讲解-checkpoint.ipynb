{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM 直方图优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直方图优化算法需要在**训练前预先把特征值转化为bin，也就是对每个特征的取值做个分段函数，将所有样本在该特征上的取值划分到某一段（bin）中**。最终把特征取值从连续值转化成了离散值。\n",
    "<br>不需要像预排序一样，把特征的所有取值进行排序）<font color='red'>谁是预排序算法的代表？---》XGBoost</font>\n",
    "<br>首先，对于当前模型的每个叶子节点，需要遍历所有的特征,目的是找到增益最大的特征及其划分值，以此来分裂该叶子节点（最后可以得到）。\n",
    "<br>对于每个特征，首先为其创建一个直方图，这个直方图存储了两类信息，分别是每个bin中样本的梯度之和（），还有就是每个bin中样本数量（）；<br>\n",
    "然后，遍历所有样本，累积上述的两类统计值到样本所属的bin中；\n",
    "<br>接着，遍历所有bin，分别以当前bin作为分割点，累加其左边的bin至当前bin的梯度和（SL）以及样本数量（nL），并与父节点上的总梯度和（Sp）以及总样本数量（np）相减，得到右边所有bin的梯度和（SR）以及样本数量（nR），带入公式，计算出增益，在遍历过程中取最大的增益，以此时的特征和bin的特征值作为分裂节点的特征和分裂特征取值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面第三步中涉及到了**lightgbm的一个优化——Histogram（直方图）做差加速**。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。利用这个方法，LightGBM 可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，**在速度上可以提升一倍**。 \n",
    "<font color='red'>还需要更直观的图。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：\n",
    "\n",
    "- 首先，最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 8 位整型存储就足够了，内存消耗可以降低为原来的1/8。 \n",
    "- 然后在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数）<font color='red'>what is K? ---》 bin</font>，时间复杂度从O(#样本数*#特征数)优化到O(k*#特征数)。 \n",
    "\n",
    "缺点：\n",
    "\n",
    "- 预处理能够忽略零值特征，减少训练代价；而直方图**不能对稀疏进行优化**，只是计算累加值（累加梯度和样本数）。但是，LightGBM对稀疏进行了优化：只用非零特征构建直方图。\n",
    "- 预排序按特征值排序，然后找划分点，可以直接跳过0值的样本，而直方图如果不单独对0值处理，这些原本为0值的样本就会合并到某一个直方图中，增加了计算量\n",
    "注意：\n",
    "- 最后一点是比较关键的一点，LightGBM为何使用直方图这种比较粗的分割节点方法，还能达到比较好的效果？\n",
    "- 虽然分割的精度变差了，但是对最后结果的影响不是很大，**主要由于决策树是弱模型， 分割点是不是精确并不是太重要** ；\n",
    "- **较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。 **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直方图优化理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.deeplearn.me/2315.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lightgbm 是微软推出的 gbdt 相关的机器学习库，一开源就受到很多开发者的喜爱吧，主要是运行速度快并且节省内存，同时训练的精度也很高，感觉集中了所有的优势。\n",
    "- 在此之前用陈天奇的 xgboost 居多，也是神器。xgboost 采用了**预排序的方法来处理节点分裂**，在计算机领域要么就是空间换时间，或者时间换空间(这个也不是绝对，你可以通过某种特殊的方法两者都可以达到你想要的效果)，**xgboost 为了加速采用了预排序的方式，那么 xgboost 处理大数据量其实是比较吃内存的**\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.deeplearn.me/wp-content/uploads/2018/09/2018090805033686.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对第2个for循环：\n",
    "- 对于当前这个特征新建一个直方图（每一个特征都有一个直方图，特征中分的段就是直方图中的bin）\n",
    "- 现在又碰到一个 for 循环了 ，这个 for 循环干的事情就是遍历所有的样本来构建直方图，哈哈，此时就用到了之前所描述的装箱操作了，直方图的每个 bin 中包含了一定的样本，在此计算每个 bin 中的样本的梯度之和并对 bin 中的样本记数。\n",
    "- 下面就是最后一个 for 循环了，这个开始遍历所有的 bin，找到适合分裂的最佳 bin，解释一下这其中涉及**到达变量定义**\n",
    "- SL\n",
    "是当前分裂 bin 左边所有 bin 的集合,对比理解SR，那么**SP其中的 P 就是 parent 的意思，就是父节点**，传统的决策树会有分裂前后信息增益的计算，典型的 ID3 或者 C45 之类，在这里我们也会计算，但是**SR中所有 bin 的梯度之和不需要在额外计算了，直接使用父节点的减去左边的就得到了（直方图算法的优化）**，是不是觉得很厉害的样子。反正我觉得这点是真的很神奇的地方，虽然看起来很简单，但是也是一个小魔法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightgbm 直方图优化算法深入理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/anshuai_aw1/article/details/83040541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在之前的介绍Xgboost的众多博文中，已经介绍过，在树分裂计算分裂特征的增益时，xgboost 采用了预排序的方法来处理节点分裂，这样计算的分裂点比较精确。但是，也造成了很大的时间开销。\n",
    "- 为了解决这个问题，Lightgbm 选择了**基于 histogram 的决策树算法**。相比于 pre-sorted算法，histogram 在内存消耗和计算代价上都有不少优势。\n",
    "- histogram算法简单来说，就是先对特征值进行装箱处理，形成一个一个的bins。对于连续特征来说，装箱处理就是特征工程中的离散化：如[0,0.3)—>0，[0.3,0.7)—->1等。在Lightgbm中默认的#bins为256（1个字节的能表示的长度，可以设置）。对于分类特征来说，则是每一种取值放入一个bin，且当取值的个数大于max bin数时，会忽略那些很少出现的category值。\n",
    "- 在节点分裂的时候，这时候就不需要按照预排序算法那样，对于每个特征都计算#data遍了，而是只需要计算#bins（第一个解释中的k）遍，这样就大大加快了训练速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "- 从算法中可以看到：直方图优化算法需要在**训练前预先把特征值转化为bin value**，也就是对每个特征的取值做个分段函数，将所有样本在该特征上的取值划分到某一段（bin）中。最终把特征取值从连续值转化成了离散值。\n",
    "- 需要注意得是：**feature value对应的bin value在整个训练过程中是不会改变的**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优点：\n",
    "- 因为 histogram 算法**仅需要存储 feature bin value (离散化后的数值)，不需要原始的 feature value，也不用排序**，而 bin value 用 1Bytes(256 bins) 的大小一般也就足够了。\n",
    "- 在计算上的优势则主要体现在“数据分割”。决策树算法有两个主要操作组成，一个是“寻找分割点”，另一个是“数据分割”。从算法时间复杂度来看，Histogram 算法和 pre-sorted 算法在“寻找分割点”的代价是一样的，都是O(#feature*#data)。\n",
    "- 而在“数据分割”时，pre-sorted 算法需要O(#feature*#data)，而 histogram 算法是O(#data)。因为 **pre-sorted 算法的每一列特征的顺序都不一样，分割的时候需要对每个特征单独进行一次分割**。\n",
    "- Histogram算法**不需要排序，所有特征共享同一个索引表**，分割的时候仅需对这个索引表操作一次就可以。（更新: 这一点不完全正确，pre-sorted 与 level-wise 结合的时候，其实可以共用一个索引表(row_idx_to_tree_node_idx)。然后在寻找分割点的时候，同时操作同一层的节点，省去分割的步骤。但这样做的问题是会有非常多随机访问，有很大的cache miss，速度依然很慢。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 举了一个例子：直方图的右子节点可以通过父节点 - 左子节点得到，节省计算量！https://blog.csdn.net/anshuai_aw1/article/details/83040541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jianshu.com/p/48e82dbb142b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 比XGBOOST更快--LightGBM介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "枚举所有不同树结构的贪心算法\n",
    "\n",
    "不断地枚举不同树的结构，根据目标函数来寻找出一个最优结构的树，加入到我们的模型中，再重复这样的操作。不过枚举所有树结构这个操作不太可行，所以常用的方法是贪心法，每一次尝试去对已有的叶子加入一个分割。对于一个具体的分割方案，我们可以获得的增益可以由如下公式计算。\n",
    "\n",
    "对于每次扩展（从上到下进行），我们还是要枚举所有可能的分割方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在每一次迭代的时候，都需要遍历整个训练数据多次（因为对每个切点，都要计算一般切点左边+右边，也就是所有数据的信息）。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。\n",
    "- 预排序方法（pre-sorted）：<font color='red'> 这个不太懂。\n",
    "\n",
    "首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。\n",
    "\n",
    "最后，对cache优化不友好。在预排序后，**特征对梯度的访问**是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 带深度限制的Leaf-wise的叶子生长策略\n",
    "- 直方图做差加速\n",
    "- 直接支持类别特征(Categorical Feature)\n",
    "- Cache命中率优化\n",
    "- 基于直方图的稀疏特征优化\n",
    "- 多线程优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 带深度限制的Leaf-wise的叶子生长策略\n",
    "\n",
    "Level-wise过一次数据可以**同时分裂同一层的叶子，容易进行多线程优化**，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。\n",
    "\n",
    "Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。（我感觉，Level-wise因为没有在每一层上都选择增益高的，而是在每个叶子上选择增益，感觉上level-wise会在每一层放弃一些数据，因为他们数据信息增益少的叶子。所以树不会很深，有助于防止过拟合。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload-images.jianshu.io/upload_images/536604-e1b3678531376e7d.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload-images.jianshu.io/upload_images/536604-fce1924d8a979e06.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 额。。所以说明LightGBM只是速度上快，但是性能上没有提升吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升算法与数据不均衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有许多成功的用例将**随机森林算法**用于高度不平衡的数据集；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了处理数据不平衡问题，使用了以下三种技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.设置类别权重/重要性：\n",
    "\n",
    "**代价敏感学习**是使随机森林更适合从非常不平衡的数据中学习的方法之一。随机森林有倾向于偏向大多数类别。因此，**对少数群体错误分类施加昂贵的惩罚可能是有作用的**。由于这种技术可以改善模型性能，所以我给少数群体分配了很高的权重（即更高的错误分类成本）。然后将类别权重合并到随机森林算法中。我根据类别1中**数据集的数量与其它数据集的数量之间的比率来确定类别权重**。例如，类别1和类别3数据集的数目之间的比率约为110，而类别1和类别2的比例约为26。现在我稍微对数量进行修改以改善模型的性能，以下代码片段显示了不同类权重的实现："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class_weight = dict({1:1.9, 2:35, 3:180})\n",
    "rdf = RandomForestClassifier(bootstrap=True,\n",
    "            class_weight=class_weight, \n",
    "            criterion='gini',\n",
    "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=4, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=300,\n",
    "            oob_score=False,\n",
    "            random_state=random_state,\n",
    "            verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.过大预测标签而不是过小预测（Over-Predict a Label than Under-Predict）：\n",
    "\n",
    "这项技术是可选的，通过实践发现，这种方法**对提高少数类别的表现非常有效**。简而言之，如果将模型错误分类为类别3，则该技术能最大限度地惩罚该模型，对于类别2和类别1惩罚力度稍差一些。 为了实施该方法，我**改变了每个类别的概率阈值，将类别3、类别2和类别1的概率设置为递增顺序（即，P3= 0.25，P2= 0.35，P1= 0.50），以便模型被迫过度预测类别。该算法的详细实现可以在Github上找到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.zhihu.com/question/40771695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过采样使得样本均衡的方法，在树模型/提升模型中依然要用。\n",
    "- 通过正负样本的惩罚权重解决样本不均衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过正负样本的惩罚权重解决样本不均衡的问题的思想是在算法实现过程中，对于分类中不同样本数量的类别分别赋予不同的权重（一般思路分类中的小样本量类别权重高，大样本量类别权重低），然后进行计算和建模。<br>\n",
    "使用这种方法时dont需要对样本本身做额外处理，只需在算法模型的参数中进行相应设置即可。很多模型和算法中都有基于类别参数的调整设置，以scikit-learn中的SVM为例，通过在**class_weight : {dict, 'balanced'}** 中针对不同类别针对不同的权重，来手动指定不同类别的权重。**如果使用其默认的方法balanced，那么SVM会将权重设置为与不同类别样本数量呈反比的权重来做自动均衡处理**，计算公式为：n_samples / (n_classes * np.bincount(y))。<br>\n",
    "如果算法本身支持，这种思路是更加简单且高效的方法。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过组合/集成方法解决样本不均衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "组合/集成方法指的是在每次生成训练集时使用所有分类中的小样本量，同时从分类中的大样本量中随机抽取数据来与小样本量合并构成训练集，这样反复多次会得到很多训练集和训练模型。最后在应用时，使用组合方法（例如投票、加权投票等）产生分类预测结果。\n",
    "<br>例如，在数据集中的正、负例的样本分别为100和10000条，比例为1:100。**此时可以将负例样本（类别中的大量样本集）随机分为100份（当然也可以分更多），每份100条数据；然后每次形成训练集时使用所有的正样本（100条）和随机抽取的负样本（100条）形成新的数据集。如此反复可以得到100个训练集和对应的训练模型。**\n",
    "<br>这种解决问题的思路类似于随机森林。在随机森林中，虽然每个小决策树的分类能力很弱，但是通过大量的“小树”组合形成的“森林”具有良好的模型预测能力。\n",
    "<br>**如果计算资源充足，并且对于模型的时效性要求不高的话，这种方法比较合适**。换句话说，就是现实中一般不用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过特征选择解决样本不均衡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述几种方法都是基于数据行的操作，通过多种途径来使得不同类别的样本数据行记录均衡。除此以外，还可以考虑使用或辅助于基于列的特征选择方法。一般情况下，样本不均衡也会导致特征分布不均衡，但如果小类别样本量具有一定的规模，那么意味着其特征值的分布较为均匀，可通过选择具有显著型的特征配合参与解决样本不均衡问题，也能在一定程度上提高模型效果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 提示 \n",
    "<br>上述几种方法的思路都是基于分类问题解决的。实际上，这种从大规模数据中寻找罕见数据的情况，也可以使用**非监督式的学习方法，例如使用One-class SVM进行异常检测**。\n",
    "<br>分类是监督式方法，前期是基于带有标签（Label）的数据进行分类预测；\n",
    "<br>而采用非监督式方法，则是**使用除了标签以外的其他特征进行模型拟合**，这样也能得到异常数据记录。所以，要解决异常检测类的问题，先是考虑整体思路，然后再考虑方法模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jianshu.com/p/3e8b9f2764c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代价敏感学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代价矩阵\n",
    "采样算法从数据层面解决不平衡数据的学习问题，**在算法层面上解决不平衡数据学习的方法主要是基于代价敏感学习算法(Cost-Sensitive Learning)** ，代价敏感学习方法的核心要素是代价矩阵，<br>\n",
    "我们注意到在实际的应用中不同类型的误分类情况导致的代价是不一样的，例如在医疗中，“将病人误疹为健康人”和“将健康人误疹为病人”的代价不同；在信用卡盗用检测中，“将盗用误认为正常使用”与“将正常使用识破认为盗用”的代价也不相同，因此我们定义代价矩阵如下图5所示。\n",
    "<br>标记$C_ij$为将类别j误分类为类别i的代价，显然$C_00=C_11=0$，$C_01,C_10$为两种不同的误分类代价，当两者相等时为代价不敏感的学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 从学习模型出发，着眼于对某一具体学习方法的改造，使之能适应不平衡数据下的学习，\n",
    "<br>研究者们针对不同的学习模型如感知机，支持向量机，决策树，神经网络等分别提出了其代价敏感的版本。\n",
    "<br>以代价敏感的决策树为例，可从三个方面对其进行改进以适应不平衡数据的学习，这三个方面分别是决策阈值的选择方面、分裂标准的选择方面、剪枝方面，这三个方面中都可以将代价矩阵引入，具体实现算法可参考参考文献中的相关文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 从**贝叶斯风险理论**出发，把代价敏感学习看成是**分类结果的一种后处理**，按照传统方法学习到一个模型，以实现损失最小为目标对结果进行调整，优化公式如下所示。此方法的**优点在于它可以不依赖所用具体的分类器，但是缺点也很明显它要求分类器输出值为概率**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 从预处理的角度出发，将代价用于权重的调整，使得分类器满足代价敏感的特性，下面讲解一种基于Adaboost的权重更新策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaCost算法\n",
    "让我们先来简单回顾一下Adaboost算法，如下图6所示。Adaboost算法通过反复迭代，每一轮迭代学习到一个分类器，并根据当前分类器的表现更新样本的权重，如图中红框所示，其更新策略为正确分类样本权重降低，错误分类样本权重加大，最终的模型是多次迭代模型的一个加权线性组合，分类越准确的分类器将会获得越大的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload-images.jianshu.io/upload_images/50828-ddd88aa21dd9bc59.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaCost算法修改了Adaboost算法的权重更新策略(多了$\\beta$)，其基本思想是对于代价高的误分类样本大大地提高其权重**，而对于代价高的正确分类样本适当地降低其权重，使其权重降低相对较小。总体思想是代价高样本权重增加得大降低得慢。其样本权重按照如下公式进行更新。其中$\\beta_+$和$\\beta_-$分别表示样本被正确和错误分类情况下$\\beta$的取值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload-images.jianshu.io/upload_images/50828-3c15fca4998dd7e5.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
